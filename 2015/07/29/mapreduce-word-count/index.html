<!doctype html>



  


<html class="theme-next mist use-motion">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />



  <meta name="google-site-verification" content="ZEgamgW9hPgdy31Xvizc7s6AvVcabD1m6d3_btcrkbA" />










  
  
  <link href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.0.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="HDFS,Hadoop,MapReduce," />





  <link rel="alternate" href="/atom.xml" title="神奕的博客" type="application/atom+xml" />




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.0.1" />






<meta name="description" content="通常我们在学习一门语言的时候，写的第一个程序就是Hello World。而在学习Hadoop时，我们要写的第一个程序就是词频统计WordCount程序。
一、MapReduce简介1.1 MapReduce编程模型MapReduce采用”分而治之”的思想，把对大规模数据集的操作，分发给一个主节点管理下的各个分节点共同完成，然后通过整合各个节点的中间结果，得到最终结果。简单地说，MapReduce就">
<meta property="og:type" content="article">
<meta property="og:title" content="第一个MapReduce程序——WordCount">
<meta property="og:url" content="http://yoursite.com/2015/07/29/mapreduce-word-count/index.html">
<meta property="og:site_name" content="神奕的博客">
<meta property="og:description" content="通常我们在学习一门语言的时候，写的第一个程序就是Hello World。而在学习Hadoop时，我们要写的第一个程序就是词频统计WordCount程序。
一、MapReduce简介1.1 MapReduce编程模型MapReduce采用”分而治之”的思想，把对大规模数据集的操作，分发给一个主节点管理下的各个分节点共同完成，然后通过整合各个节点的中间结果，得到最终结果。简单地说，MapReduce就">
<meta property="og:image" content="http://img.blog.csdn.net/20150728184403091">
<meta property="og:image" content="http://img.blog.csdn.net/20150729173938301">
<meta property="og:image" content="http://img.blog.csdn.net/20150729174039812">
<meta property="og:image" content="http://img.blog.csdn.net/20150729174057622">
<meta property="og:image" content="http://img.blog.csdn.net/20150729174400615">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="第一个MapReduce程序——WordCount">
<meta name="twitter:description" content="通常我们在学习一门语言的时候，写的第一个程序就是Hello World。而在学习Hadoop时，我们要写的第一个程序就是词频统计WordCount程序。
一、MapReduce简介1.1 MapReduce编程模型MapReduce采用”分而治之”的思想，把对大规模数据集的操作，分发给一个主节点管理下的各个分节点共同完成，然后通过整合各个节点的中间结果，得到最终结果。简单地说，MapReduce就">



<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: 0,
      author: '博主'
    }
  };
</script>




  <link rel="canonical" href="http://yoursite.com/2015/07/29/mapreduce-word-count/"/>

  <title> 第一个MapReduce程序——WordCount | 神奕的博客 </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  










  
  
    
  

  <div class="container one-collumn sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">神奕的博客</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle">李松</p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      

      
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                第一个MapReduce程序——WordCount
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2015-07-29T21:34:39+08:00" content="2015-07-29">
              2015-07-29
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/大数据-Hadoop/" itemprop="url" rel="index">
                    <span itemprop="name">大数据-Hadoop</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2015/07/29/mapreduce-word-count/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2015/07/29/mapreduce-word-count/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>通常我们在学习一门语言的时候，写的第一个程序就是Hello World。而在学习Hadoop时，我们要写的第一个程序就是词频统计<code>WordCount</code>程序。</p>
<h1 id="一、MapReduce简介">一、MapReduce简介</h1><h2 id="1-1_MapReduce编程模型">1.1 MapReduce编程模型</h2><p>MapReduce采用”分而治之”的思想，把对大规模数据集的操作，分发给一个主节点管理下的各个分节点共同完成，然后通过整合各个节点的中间结果，得到最终结果。简单地说，MapReduce就是”任务的分解与结果的汇总”。</p>
<p>在Hadoop中，用于执行MapReduce任务的机器角色有两个：<a id="more"></a></p>
<ul>
<li>JobTracker用于调度工作的，一个Hadoop集群中只有一个JobTracker，位于master。</li>
<li>TaskTracker用于执行工作，位于各slave上。</li>
</ul>
<p>在分布式计算中，MapReduce框架负责处理了并行编程中分布式存储、工作调度、负载均衡、容错均衡、容错处理以及网络通信等复杂问题，把处理过程高度抽象为两个函数：map和reduce，map负责把任务分解成多个任务，reduce负责把分解后多任务处理的结果汇总起来。</p>
<p>需要注意的是，用MapReduce来处理的数据集（或任务）必须具备这样的特点：待处理的数据集可以分解成许多小的数据集，而且每一个小数据集都可以完全并行地进行处理。</p>
<h2 id="1-2_MapReduce处理过程">1.2 MapReduce处理过程</h2><p>在Hadoop中，每个MapReduce任务都被初始化为一个Job，每个Job又可以分为两种阶段：map阶段和reduce阶段。</p>
<ul>
<li>map：<code>(K1, V1)</code> ——&gt; <code>list(K2, V2)</code></li>
<li>reduce：<code>(K2, list(V2))</code> ——&gt; <code>list(K3, V3)</code></li>
</ul>
<p>如下图所示：</p>
<p><img src="http://img.blog.csdn.net/20150728184403091" alt=""></p>
<p><br></p>
<h1 id="二、运行WordCount程序">二、运行WordCount程序</h1><p>在运行程序之前，需要先搭建好Hadoop集群环境，参考《<a href="http://blog.csdn.net/lisonglisonglisong/article/details/46974723" target="_blank" rel="external">Hadoop+HBase+ZooKeeper分布式集群环境搭建</a>》。</p>
<h2 id="2-1_源代码">2.1 源代码</h2><p>WordCount可以说是最简单的MapReduce程序了，只包含三个文件：一个 Map 的 Java 文件，一个 Reduce 的 Java 文件，一个负责调用的主程序 Java 文件。</p>
<p>我们在当前用户的主文件夹下创建<code>wordcount_01/</code>目录，在该目录下再创建<code>src/</code>和<code>classes/</code>。 src 目录存放 Java 的源代码，classes 目录存放编译结果。</p>
<p><strong>TokenizerMapper.java</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.lisong.hadoop;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.StringTokenizer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TokenizerMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">Object</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">	IntWritable one = <span class="keyword">new</span> IntWritable(<span class="number">1</span>);</span><br><span class="line">	Text word = <span class="keyword">new</span> Text();</span><br><span class="line">	</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(Object key, Text value, Context context)</span> <span class="keyword">throws</span> IOException,InterruptedException </span>&#123;</span><br><span class="line">		StringTokenizer itr = <span class="keyword">new</span> StringTokenizer(value.toString());</span><br><span class="line">		<span class="keyword">while</span>(itr.hasMoreTokens()) &#123;</span><br><span class="line">			word.set(itr.nextToken());</span><br><span class="line">			context.write(word, one);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>IntSumReducer.java</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.lisong.hadoop;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">IntSumReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">	IntWritable result = <span class="keyword">new</span> IntWritable();</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text	key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException,InterruptedException </span>&#123;</span><br><span class="line">		<span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">		<span class="keyword">for</span>(IntWritable val:values) &#123;</span><br><span class="line">			sum += val.get();</span><br><span class="line">		&#125;</span><br><span class="line">		result.set(sum);</span><br><span class="line">		context.write(key,result);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>WordCount.java</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.lisong.hadoop;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.GenericOptionsParser;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCount</span> </span>&#123;</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">		Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">		String[] otherArgs = <span class="keyword">new</span> GenericOptionsParser(conf, args).getRemainingArgs();</span><br><span class="line">		<span class="keyword">if</span>(otherArgs.length != <span class="number">2</span>) &#123;</span><br><span class="line">			System.err.println(<span class="string">"Usage: wordcount &lt;in&gt; &lt;out&gt;"</span>);</span><br><span class="line">			System.exit(<span class="number">2</span>);</span><br><span class="line">		&#125;</span><br><span class="line">		</span><br><span class="line">		Job job = <span class="keyword">new</span> Job(conf, <span class="string">"wordcount"</span>);</span><br><span class="line">		job.setJarByClass(WordCount.class);</span><br><span class="line">		job.setMapperClass(TokenizerMapper.class);</span><br><span class="line">		job.setCombinerClass(IntSumReducer.class);</span><br><span class="line">		job.setReducerClass(IntSumReducer.class);</span><br><span class="line">		job.setOutputKeyClass(Text.class);</span><br><span class="line">		job.setOutputValueClass(IntWritable.class);</span><br><span class="line">		FileInputFormat.addInputPath(job, <span class="keyword">new</span> Path(otherArgs[<span class="number">0</span>]));</span><br><span class="line">		FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(otherArgs[<span class="number">1</span>]));</span><br><span class="line">		System.exit(job.waitForCompletion(<span class="keyword">true</span>)?<span class="number">0</span>:<span class="number">1</span>);</span><br><span class="line">	&#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>以上三个.java源文件均置于 src 目录下。</p>
<h2 id="2-2_编译">2.2 编译</h2><p>Hadoop 2.x 版本中jar不再集中在一个 hadoop-core-*.jar 中，而是分成多个 jar。编译WordCount程序需要如下三个 jar：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$HADOOP_HOME</span>/share/hadoop/common/hadoop-common-<span class="number">2.4</span>.<span class="number">1</span><span class="class">.jar</span></span><br><span class="line"><span class="variable">$HADOOP_HOME</span>/share/hadoop/mapreduce/hadoop-mapreduce-client-core-<span class="number">2.4</span>.<span class="number">1</span><span class="class">.jar</span></span><br><span class="line"><span class="variable">$HADOOP_HOME</span>/share/hadoop/common/lib/commons-cli-<span class="number">1.2</span>.jar</span><br></pre></td></tr></table></figure></p>
<p>使用<code>javac</code>命令进行编译：<br><figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ cd wordcount_01</span><br><span class="line"></span><br><span class="line">$ javac -<span class="keyword">classpath</span> <span class="regexp">/home/</span>hadoop<span class="regexp">/hadoop/</span>share<span class="regexp">/hadoop/</span>common<span class="regexp">/hadoop-common-2.6.0.jar:/</span>home<span class="regexp">/hadoop/</span>hadoop<span class="regexp">/share/</span>hadoop<span class="regexp">/mapreduce/</span>hadoop-mapreduce-client-core-<span class="number">2.6</span>.<span class="number">0</span>.jar:<span class="regexp">/home/</span>hadoop<span class="regexp">/hadoop/</span>share<span class="regexp">/hadoop/</span>common<span class="regexp">/lib/</span>commons-cli-<span class="number">1.2</span>.jar -d classes<span class="regexp">/ src/</span>*.java</span><br></pre></td></tr></table></figure></p>
<ul>
<li>-classpath，设置源代码里使用的各种类库所在的路径，多个路径用<code>&quot;:&quot;</code>隔开。</li>
<li>-d，设置编译后的 class 文件保存的路径。</li>
<li>src/*.java，待编译的源文件。</li>
</ul>
<h2 id="2-3_打包">2.3 打包</h2><p>将编译好的 class 文件打包成 Jar 包，jar 命令是 JDK 的打包命令行工具。</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ jar -cvf wordcount<span class="class">.jar</span> classes</span><br></pre></td></tr></table></figure>
<p>打包结果是 wordcount.jar 文件，放在当前目录下。</p>
<h2 id="2-4_执行">2.4 执行</h2><p>执行hadoop程序的时候，输入文件必须先放入hdfs文件系统中，不能是本地文件。</p>
<p><strong>1 . 先查看hdfs文件系统的根目录：</strong><br><figure class="highlight coffeescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop<span class="regexp">/bin/hadoop fs -ls /</span></span><br><span class="line">Found <span class="number">1</span> items</span><br><span class="line">drwxr-xr-x   - hadoop supergroup          <span class="number">0</span> <span class="number">2015</span>-<span class="number">07</span>-<span class="number">28</span> <span class="number">14</span>:<span class="number">38</span> /hbase</span><br></pre></td></tr></table></figure></p>
<p>可以看出，hdfs的根目录是一个叫<code>/hbase</code>的目录。</p>
<p><strong>2 . 然后利用<code>put</code>将输入文件（多个输入文件位于<code>input</code>文件夹下）复制到hdfs文件系统中：</strong><br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop/bin/hadoop fs -put <span class="tag">input</span> /hbase</span><br></pre></td></tr></table></figure></p>
<p><strong>3 . 运行wordcount程序</strong></p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop/bin/hadoop jar wordcount_01/wordcount<span class="class">.jar</span> WordCount /hbase/<span class="tag">input</span> /hbase/output</span><br></pre></td></tr></table></figure>
<p>提示找不到 WordCount 类：<code>Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: WordCount</code>…</p>
<p>因为程序中声明了 package ，所以在命令中也要 com.lisong.hadoop.WordCount 写完整：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop/bin/hadoop jar wordcount_01/wordcount<span class="class">.jar</span> com<span class="class">.lisong</span><span class="class">.hadoop</span><span class="class">.WordCount</span> /hbase/<span class="tag">input</span> /hbase/output</span><br></pre></td></tr></table></figure></p>
<p>其中 “jar” 参数是指定 jar 包的位置，com.lisong.hadoop.WordCount 是主类。运行程序处理 input 目录下的多个文件，将结果写入 /hbase/output 目录。</p>
<p><strong>4 . 查看运行结果</strong></p>
<figure class="highlight brainfuck"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">$</span> <span class="comment">hadoop/bin/hadoop</span> <span class="comment">fs</span> <span class="literal">-</span><span class="comment">ls</span> <span class="comment">/hbase/output</span></span><br><span class="line"><span class="comment">Found</span> <span class="comment">2</span> <span class="comment">items</span></span><br><span class="line"><span class="literal">-</span><span class="comment">rw</span><span class="literal">-</span><span class="comment">r</span><span class="literal">-</span><span class="literal">-</span><span class="comment">r</span><span class="literal">-</span><span class="literal">-</span>   <span class="comment">3</span> <span class="comment">hadoop</span> <span class="comment">supergroup</span>          <span class="comment">0</span> <span class="comment">2015</span><span class="literal">-</span><span class="comment">07</span><span class="literal">-</span><span class="comment">28</span> <span class="comment">18:05</span> <span class="comment">/hbase/output/_SUCCESS</span></span><br><span class="line"><span class="literal">-</span><span class="comment">rw</span><span class="literal">-</span><span class="comment">r</span><span class="literal">-</span><span class="literal">-</span><span class="comment">r</span><span class="literal">-</span><span class="literal">-</span>   <span class="comment">3</span> <span class="comment">hadoop</span> <span class="comment">supergroup</span>         <span class="comment">33</span> <span class="comment">2015</span><span class="literal">-</span><span class="comment">07</span><span class="literal">-</span><span class="comment">28</span> <span class="comment">18:05</span> <span class="comment">/hbase/output/part</span><span class="literal">-</span><span class="comment">r</span><span class="literal">-</span><span class="comment">00000</span></span><br></pre></td></tr></table></figure>
<p>可以看到<code>/hbase/output/</code>目录下有两个文件，结果就存在<code>part-r-00000</code>中：<br><figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop<span class="regexp">/bin/</span>hadoop fs -cat <span class="regexp">/hbase/</span>output/part-r-<span class="number">00000</span></span><br><span class="line">Google	<span class="number">6</span></span><br><span class="line">Java	<span class="number">2</span></span><br><span class="line">baidu	<span class="number">3</span></span><br><span class="line">hadoop	<span class="number">4</span></span><br></pre></td></tr></table></figure></p>
<p><br></p>
<h1 id="三、WordCount程序分析">三、WordCount程序分析</h1><h2 id="3-1_Hadoop数据类型">3.1 Hadoop数据类型</h2><p>Hadoop MapReduce操作的是键值对，但这些键值对并不是Integer、String等标准的Java类型。为了让键值对可以在集群上移动，Hadoop提供了一些实现了<code>WritableComparable</code>接口的基本数据类型，以便用这些类型定义的数据可以被<strong>序列化</strong>进行网络传输、文件存储与大小比较。</p>
<ul>
<li>值：仅会被简单的传递，必须实现<code>Writable</code>或<code>WritableComparable</code>接口。</li>
<li>键：在Reduce阶段排序时需要进行比较，故只能实现<code>WritableComparable</code>接口。</li>
</ul>
<p>下面是8个预定义的Hadoop基本数据类型，它们均实现了<code>WritableComparable</code>接口：</p>
<table>
<thead>
<tr>
<th>类</th>
<th style="text-align:left">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>BooleanWritable</td>
<td style="text-align:left">标准布尔型数值</td>
</tr>
<tr>
<td>ByteWritable</td>
<td style="text-align:left">单字节数值</td>
</tr>
<tr>
<td>DoubleWritable</td>
<td style="text-align:left">双字节数</td>
</tr>
<tr>
<td>FloatWritable</td>
<td style="text-align:left">浮点数</td>
</tr>
<tr>
<td>IntWritable</td>
<td style="text-align:left">整型数</td>
</tr>
<tr>
<td>LongWritable</td>
<td style="text-align:left">长整型数</td>
</tr>
<tr>
<td>Text</td>
<td style="text-align:left">使用UTF8格式存储的文本</td>
</tr>
<tr>
<td>NullWritable</td>
<td style="text-align:left">当<code>&lt;key,value&gt;</code>中的key或value为空时使用</td>
</tr>
</tbody>
</table>
<h2 id="3-2_源代码分析">3.2 源代码分析</h2><p><strong>3.2.1 Map过程</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.lisong.hadoop;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.StringTokenizer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TokenizerMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">Object</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">	IntWritable one = <span class="keyword">new</span> IntWritable(<span class="number">1</span>);</span><br><span class="line">	Text word = <span class="keyword">new</span> Text();</span><br><span class="line">	</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(Object key, Text value, Context context)</span> <span class="keyword">throws</span> IOException,InterruptedException </span>&#123;</span><br><span class="line">		StringTokenizer itr = <span class="keyword">new</span> StringTokenizer(value.toString());</span><br><span class="line">		<span class="keyword">while</span>(itr.hasMoreTokens()) &#123;</span><br><span class="line">			word.set(itr.nextToken());</span><br><span class="line">			context.write(word, one);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Map过程需要继承<code>org.apache.hadoop.mapreduce</code>包中 Mapper 类，并<strong>重写</strong>其map方法。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TokenizerMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">Object</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>其中的模板参数：第一个Object表示输入key的类型；第二个Text表示输入value的类型；第三个Text表示表示输出键的类型；第四个IntWritable表示输出值的类型。</p>
<p>作为map方法输入的键值对，其value值存储的是文本文件中的一行（以回车符为行结束标记），而key值为该行的首字母相对于文本文件的首地址的偏移量。然后StringTokenizer类将每一行拆分成为一个个的单词，并将<code>&lt;word,1&gt;</code>作为map方法的结果输出，其余的工作都交有 MapReduce框架 处理。</p>
<p><strong>注：</strong><code>StringTokenizer</code>是Java工具包中的一个类，用于将字符串进行拆分——默认情况下使用空格作为分隔符进行分割。</p>
<p><strong>3.2.2 Reduce过程</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.lisong.hadoop;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">IntSumReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">	IntWritable result = <span class="keyword">new</span> IntWritable();</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text	key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException,InterruptedException </span>&#123;</span><br><span class="line">		<span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">		<span class="keyword">for</span>(IntWritable val:values) &#123;</span><br><span class="line">			sum += val.get();</span><br><span class="line">		&#125;</span><br><span class="line">		result.set(sum);</span><br><span class="line">		context.write(key,result);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Reduce过程需要继承<code>org.apache.hadoop.mapreduce</code>包中 Reducer 类，并 <strong>重写</strong> reduce方法。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">IntSumReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt;</span></span><br></pre></td></tr></table></figure></p>
<p>其中模板参数同Map一样，依次表示是输入键类型，输入值类型，输出键类型，输出值类型。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text	key, Iterable&lt;IntWritable&gt; values, Context context)</span></span></span><br></pre></td></tr></table></figure>
<p>reduce 方法的输入参数 key 为单个单词，而 values 是由各Mapper上对应单词的计数值所组成的列表（一个实现了 Iterable 接口的变量，可以理解成 values 里包含若干个 IntWritable 整数，可以通过迭代的方式遍历所有的值），所以只要遍历 values 并求和，即可得到某个单词出现的总次数。</p>
<p><strong>3.2.3 执行作业</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.lisong.hadoop;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.GenericOptionsParser;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCount</span> </span>&#123;</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">		Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">		String[] otherArgs = <span class="keyword">new</span> GenericOptionsParser(conf, args).getRemainingArgs();</span><br><span class="line">		<span class="keyword">if</span>(otherArgs.length != <span class="number">2</span>) &#123;</span><br><span class="line">			System.err.println(<span class="string">"Usage: wordcount &lt;in&gt; &lt;out&gt;"</span>);</span><br><span class="line">			System.exit(<span class="number">2</span>);</span><br><span class="line">		&#125;</span><br><span class="line">		</span><br><span class="line">		Job job = <span class="keyword">new</span> Job(conf, <span class="string">"wordcount"</span>);</span><br><span class="line">		job.setJarByClass(WordCount.class);</span><br><span class="line">		job.setMapperClass(TokenizerMapper.class);</span><br><span class="line">		job.setCombinerClass(IntSumReducer.class);</span><br><span class="line">		job.setReducerClass(IntSumReducer.class);</span><br><span class="line">		job.setOutputKeyClass(Text.class);</span><br><span class="line">		job.setOutputValueClass(IntWritable.class);</span><br><span class="line">		FileInputFormat.addInputPath(job, <span class="keyword">new</span> Path(otherArgs[<span class="number">0</span>]));</span><br><span class="line">		FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(otherArgs[<span class="number">1</span>]));</span><br><span class="line">		System.exit(job.waitForCompletion(<span class="keyword">true</span>)?<span class="number">0</span>:<span class="number">1</span>);</span><br><span class="line">	&#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在MapReduce中，由Job对象负责管理和运行一个计算任务，并通过Job的一些方法对任务的参数进行相关的设置，此处：</p>
<ul>
<li>设置了使用<code>TokenizerMapper.class</code>完成Map过程中的处理，使用<code>IntSumReducer.class</code>完成Combine和Reduce过程中的处理。</li>
<li>还设置了Map过程和Reduce过程的输出类型：key的类型为Text，value的类型为IntWritable。</li>
<li><p>任务的输出和输入路径则由命令行参数指定，并由FileInputFormat和FileOutputFormat分别设定。</p>
<ol>
<li>FileInputFormat类的很重要的作用就是将文件进行切分 split，并将 split 进一步拆分成key/value对</li>
<li>FileOutputFormat类的作用是将处理结果写入输出文件。</li>
</ol>
</li>
<li><p>完成相应任务的参数设定后，即可调用 <code>job.waitForCompletion()</code> 方法执行任务。</p>
</li>
</ul>
<p><strong>3.2.4 WordCount流程</strong></p>
<p>1）将文件拆分成splits，由于测试用的文件较小，所以每个文件为一个split，并将文件按行分割形成<code>&lt;key,value&gt;</code>对，key为偏移量（包括了回车符），value为文本行。这一步由MapReduce框架自动完成，如下图：</p>
<p><img src="http://img.blog.csdn.net/20150729173938301" alt=""></p>
<p>2）将分割好的<code>&lt;key,value&gt;</code>对交给用户定义的map方法进行处理，生成新的<code>&lt;key,value&gt;</code>对，如下图所示：</p>
<p><img src="http://img.blog.csdn.net/20150729174039812" alt=""></p>
<p>3）得到map方法输出的<code>&lt;key,value&gt;</code>对后，Mapper会将它们按照key值进行排序，并执行Combine过程，将key值相同的value值累加，得到Mapper的最终输出结果。如下图：</p>
<p><img src="http://img.blog.csdn.net/20150729174057622" alt=""></p>
<p>4）Reducer先对从Mapper接收的数据进行排序，再交由用户自定义的reduce方法进行处理，得到新的<code>&lt;key,value&gt;</code>对，并作为WordCount的输出结果，如下图：</p>
<p><img src="http://img.blog.csdn.net/20150729174400615" alt=""></p>
<p><br><br><br><br><br></p>
<hr>
<p>参考： 实战Hadoop：开启通向云计算的捷径</p>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/HDFS/" rel="tag">#HDFS</a>
          
            <a href="/tags/Hadoop/" rel="tag">#Hadoop</a>
          
            <a href="/tags/MapReduce/" rel="tag">#MapReduce</a>
          
        </div>
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2015/07/24/hbase-introduction/" rel="next" title="HBase技术简介">
                <i class="fa fa-chevron-left"></i> HBase技术简介
              </a>
            
          </div>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2015/08/13/hdfs-import-to-hbase/" rel="prev" title="MapReduce将HDFS文本数据导入HBase中">
                MapReduce将HDFS文本数据导入HBase中 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
      <div class="ds-thread" data-thread-key="2015/07/29/mapreduce-word-count/"
           data-title="第一个MapReduce程序——WordCount" data-url="http://yoursite.com/2015/07/29/mapreduce-word-count/">
      </div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="https://avatars0.githubusercontent.com/u/6904366?v=3&s=140"
               alt="Song Lee" />
          <p class="site-author-name" itemprop="name">Song Lee</p>
          <p class="site-description motion-element" itemprop="description">放宽心，多努力</p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">88</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          
            <div class="site-state-item site-state-categories">
              <a href="/categories">
                <span class="site-state-item-count">17</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">25</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/SongLee24" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://blog.csdn.net/lisonglisonglisong" target="_blank" title="CSDN">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                  CSDN
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://weibo.com/lisonglisong" target="_blank" title="Weibo">
                  
                    <i class="fa fa-fw fa-weibo"></i>
                  
                  Weibo
                </a>
              </span>
            
          
        </div>

        
        
          <div class="cc-license motion-element" itemprop="license">
            <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" target="_blank">
              <img src="/images/cc-by-nc-sa.svg" alt="Creative Commons" />
            </a>
          </div>
        

        
        

      </section>

      
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">
            
              
            
            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#一、MapReduce简介"><span class="nav-number">1.</span> <span class="nav-text">一、MapReduce简介</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-1_MapReduce编程模型"><span class="nav-number">1.1.</span> <span class="nav-text">1.1 MapReduce编程模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-2_MapReduce处理过程"><span class="nav-number">1.2.</span> <span class="nav-text">1.2 MapReduce处理过程</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#二、运行WordCount程序"><span class="nav-number">2.</span> <span class="nav-text">二、运行WordCount程序</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1_源代码"><span class="nav-number">2.1.</span> <span class="nav-text">2.1 源代码</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-2_编译"><span class="nav-number">2.2.</span> <span class="nav-text">2.2 编译</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-3_打包"><span class="nav-number">2.3.</span> <span class="nav-text">2.3 打包</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-4_执行"><span class="nav-number">2.4.</span> <span class="nav-text">2.4 执行</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#三、WordCount程序分析"><span class="nav-number">3.</span> <span class="nav-text">三、WordCount程序分析</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#3-1_Hadoop数据类型"><span class="nav-number">3.1.</span> <span class="nav-text">3.1 Hadoop数据类型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-2_源代码分析"><span class="nav-number">3.2.</span> <span class="nav-text">3.2 源代码分析</span></a></li></ol></li></ol></div>
            
          </div>
        </section>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy;  2014 - 
  <span itemprop="copyrightYear">2016</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Song Lee</span>
</div>

<div class="powered-by">
  <div style="float:left;margin-top:7px;margin-right:10px;">
	<script type="text/javascript">var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan id='cnzz_stat_icon_1254974724'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s11.cnzz.com/z_stat.php%3Fid%3D1254974724%26show%3Dpic' type='text/javascript'%3E%3C/script%3E"));</script>
  </div>
</div>

<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>

        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/vendors/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.0.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.0.1"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.0.1"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.0.1"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.0.1"></script>



  

  
    
  

  <script type="text/javascript">
    var duoshuoQuery = {short_name:"songlee24"};
    (function() {
      var ds = document.createElement('script');
      ds.type = 'text/javascript';ds.async = true;
      ds.id = 'duoshuo-script';
      ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
      ds.charset = 'UTF-8';
      (document.getElementsByTagName('head')[0]
      || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
  </script>

  
    
    <script src="/vendors/ua-parser-js/dist/ua-parser.min.js?v=0.7.9"></script>
    <script src="/js/src/hook-duoshuo.js"></script>
  






  
  

  

  

  

</body>
</html>
