<!doctype html>



  


<html class="theme-next mist use-motion">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />



  <meta name="google-site-verification" content="ZEgamgW9hPgdy31Xvizc7s6AvVcabD1m6d3_btcrkbA" />










  
  
  <link href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.0.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="HBase,HDFS,Hadoop,MapReduce," />





  <link rel="alternate" href="/atom.xml" title="神奕的博客" type="application/atom+xml" />




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.0.1" />






<meta name="description" content="HBase本身提供了很多种数据导入的方式，通常有两种常用方式：

使用HBase提供的TableOutputFormat，原理是通过一个Mapreduce作业将数据导入HBase
另一种方式就是使用HBase原生Client API

本文就是示范如何通过MapReduce作业从一个文件读取数据并写入到HBase中。
首先启动Hadoop与HBase，然后创建一个空表，用于后面导入数据：">
<meta property="og:type" content="article">
<meta property="og:title" content="MapReduce将HDFS文本数据导入HBase中">
<meta property="og:url" content="http://yoursite.com/2015/08/13/hdfs-import-to-hbase/index.html">
<meta property="og:site_name" content="神奕的博客">
<meta property="og:description" content="HBase本身提供了很多种数据导入的方式，通常有两种常用方式：

使用HBase提供的TableOutputFormat，原理是通过一个Mapreduce作业将数据导入HBase
另一种方式就是使用HBase原生Client API

本文就是示范如何通过MapReduce作业从一个文件读取数据并写入到HBase中。
首先启动Hadoop与HBase，然后创建一个空表，用于后面导入数据：">
<meta property="og:image" content="http://img.blog.csdn.net/20150812105912587">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="MapReduce将HDFS文本数据导入HBase中">
<meta name="twitter:description" content="HBase本身提供了很多种数据导入的方式，通常有两种常用方式：

使用HBase提供的TableOutputFormat，原理是通过一个Mapreduce作业将数据导入HBase
另一种方式就是使用HBase原生Client API

本文就是示范如何通过MapReduce作业从一个文件读取数据并写入到HBase中。
首先启动Hadoop与HBase，然后创建一个空表，用于后面导入数据：">



<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: 0,
      author: '博主'
    }
  };
</script>




  <link rel="canonical" href="http://yoursite.com/2015/08/13/hdfs-import-to-hbase/"/>

  <title> MapReduce将HDFS文本数据导入HBase中 | 神奕的博客 </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  










  
  
    
  

  <div class="container one-collumn sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">神奕的博客</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle">李松</p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      

      
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                MapReduce将HDFS文本数据导入HBase中
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2015-08-13T07:40:33+08:00" content="2015-08-13">
              2015-08-13
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/大数据-HBase/" itemprop="url" rel="index">
                    <span itemprop="name">大数据-HBase</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2015/08/13/hdfs-import-to-hbase/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2015/08/13/hdfs-import-to-hbase/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>HBase本身提供了很多种数据导入的方式，通常有两种常用方式：</p>
<ol>
<li>使用HBase提供的TableOutputFormat，原理是通过一个Mapreduce作业将数据导入HBase</li>
<li>另一种方式就是使用HBase原生Client API</li>
</ol>
<p>本文就是示范如何通过MapReduce作业从一个文件读取数据并写入到HBase中。</p>
<p>首先启动Hadoop与HBase，然后创建一个空表，用于后面导入数据：<a id="more"></a><br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="title">hbase</span><span class="params">(main)</span></span>:<span class="number">006</span>:<span class="number">0</span>&gt; create <span class="string">'mytable'</span>,<span class="string">'cf'</span></span><br><span class="line"><span class="number">0</span> <span class="function"><span class="title">row</span><span class="params">(s)</span></span> <span class="keyword">in</span> <span class="number">10.8310</span> seconds</span><br><span class="line"></span><br><span class="line">=&gt; Hbase::Table - mytable</span><br><span class="line"><span class="function"><span class="title">hbase</span><span class="params">(main)</span></span>:<span class="number">007</span>:<span class="number">0</span>&gt; list</span><br><span class="line">TABLE                                                                                                   </span><br><span class="line">mytable                                                                                                 </span><br><span class="line"><span class="number">1</span> <span class="function"><span class="title">row</span><span class="params">(s)</span></span> <span class="keyword">in</span> <span class="number">0.1220</span> seconds</span><br><span class="line"></span><br><span class="line">=&gt; [<span class="string">"mytable"</span>]</span><br><span class="line"><span class="function"><span class="title">hbase</span><span class="params">(main)</span></span>:<span class="number">008</span>:<span class="number">0</span>&gt; scan <span class="string">'mytable'</span></span><br><span class="line">ROW                         COLUMN+CELL                                                                 </span><br><span class="line"><span class="number">0</span> <span class="function"><span class="title">row</span><span class="params">(s)</span></span> <span class="keyword">in</span> <span class="number">0.2130</span> seconds</span><br></pre></td></tr></table></figure></p>
<h1 id="一、示例程序">一、示例程序</h1><p>下面的示例程序通过<code>TableOutputFormat</code>将HDFS上具有一定格式的文本数据导入到HBase中。</p>
<p>首先创建MapReduce作业，目录结构如下：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Hdfs2HBase/</span><br><span class="line">├── classes</span><br><span class="line">└── src</span><br><span class="line">    ├── Hdfs2HBase<span class="class">.java</span></span><br><span class="line">    ├── Hdfs2HBaseMapper<span class="class">.java</span></span><br><span class="line">    └── Hdfs2HBaseReducer.java</span><br></pre></td></tr></table></figure></p>
<p><strong>Hdfs2HBaseMapper.java</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.lisong.hdfs2hbase;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Hdfs2HBaseMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text line, Context context)</span> <span class="keyword">throws</span> IOException,InterruptedException </span>&#123;</span><br><span class="line">                String lineStr = line.toString();</span><br><span class="line">                <span class="keyword">int</span> index = lineStr.indexOf(<span class="string">":"</span>);</span><br><span class="line">                String rowkey = lineStr.substring(<span class="number">0</span>, index);</span><br><span class="line">                String left = lineStr.substring(index+<span class="number">1</span>);</span><br><span class="line">                context.write(<span class="keyword">new</span> Text(rowkey), <span class="keyword">new</span> Text(left));</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>Hdfs2HBaseReducer.java</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.lisong.hdfs2hbase;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.io.ImmutableBytesWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.Put;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.util.Bytes;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Hdfs2HBaseReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">Text</span>, <span class="title">ImmutableBytesWritable</span>, <span class="title">Put</span>&gt; </span>&#123;</span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text rowkey, Iterable&lt;Text&gt; value, Context context)</span> <span class="keyword">throws</span> IOException,InterruptedException </span>&#123;</span><br><span class="line">                String k = rowkey.toString();</span><br><span class="line">                <span class="keyword">for</span>(Text val : value) &#123;</span><br><span class="line">                        Put put = <span class="keyword">new</span> Put(k.getBytes());</span><br><span class="line">                        String[] strs = val.toString().split(<span class="string">":"</span>);</span><br><span class="line">                        String family = strs[<span class="number">0</span>];</span><br><span class="line">                        String qualifier = strs[<span class="number">1</span>];</span><br><span class="line">                        String v = strs[<span class="number">2</span>];</span><br><span class="line">                        put.add(family.getBytes(), qualifier.getBytes(), v.getBytes());</span><br><span class="line">                        context.write(<span class="keyword">new</span> ImmutableBytesWritable(k.getBytes()), put);</span><br><span class="line">                &#125;</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>Hdfs2HBase.java</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.lisong.hdfs2hbase;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.io.ImmutableBytesWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.Put;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.mapreduce.TableOutputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.GenericOptionsParser;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Hdfs2HBase</span> </span>&#123;</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">		Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">		String[] otherArgs = <span class="keyword">new</span> GenericOptionsParser(conf, args).getRemainingArgs();</span><br><span class="line">		<span class="keyword">if</span>(otherArgs.length != <span class="number">2</span>) &#123;</span><br><span class="line">			System.err.println(<span class="string">"Usage: wordcount &lt;infile&gt; &lt;table&gt;"</span>);</span><br><span class="line">			System.exit(<span class="number">2</span>);</span><br><span class="line">		&#125;</span><br><span class="line">		</span><br><span class="line">		Job job = <span class="keyword">new</span> Job(conf, <span class="string">"hdfs2hbase"</span>);</span><br><span class="line">		job.setJarByClass(Hdfs2HBase.class);</span><br><span class="line">		job.setMapperClass(Hdfs2HBaseMapper.class);</span><br><span class="line">		job.setReducerClass(Hdfs2HBaseReducer.class);</span><br><span class="line">		</span><br><span class="line">		job.setOutputKeyClass(ImmutableBytesWritable.class);</span><br><span class="line">		job.setOutputValueClass(Put.class);</span><br><span class="line">		</span><br><span class="line">		job.setOutputFormatClass(TableOutputFormat.class);</span><br><span class="line">		</span><br><span class="line">		FileInputFormat.addInputPath(job, <span class="keyword">new</span> Path(otherArgs[<span class="number">0</span>]));</span><br><span class="line">		job.getConfiguration().set(TableOutputFormat.OUTPUT_TABLE, otherArgs[<span class="number">1</span>]);</span><br><span class="line">		</span><br><span class="line">		System.exit(job.waitForCompletion(<span class="keyword">true</span>)?<span class="number">0</span>:<span class="number">1</span>);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>配置<code>javac</code>编译依赖环境：</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$HADOOP_HOME</span>/share/hadoop/common/hadoop-common-<span class="number">2.4</span>.<span class="number">1</span><span class="class">.jar</span></span><br><span class="line"><span class="variable">$HADOOP_HOME</span>/share/hadoop/mapreduce/hadoop-mapreduce-client-core-<span class="number">2.4</span>.<span class="number">1</span><span class="class">.jar</span></span><br><span class="line"><span class="variable">$HADOOP_HOME</span>/share/hadoop/common/lib/commons-cli-<span class="number">1.2</span>.jar</span><br></pre></td></tr></table></figure>
<p>这里要操作HBase，故除了上面三个jar包，还需要<code>$HBASE_HOME/lib</code>目录下的jar包。为了方便，我们在<code>/etc/profile</code>的<strong><code>CLASSPATH</code></strong>里包含所有的依赖包：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">TEMP=`ls /home/hadoop/hbase/lib/*.jar`</span><br><span class="line">HBASE_JARS=`<span class="built_in">echo</span> <span class="variable">$TEMP</span> | sed <span class="string">'s/ /:/g'</span>`</span><br><span class="line"><span class="built_in">export</span> CLASSPATH=.:<span class="variable">$JAVA_HOME</span>/lib/dt.jar:<span class="variable">$JAVA_HOME</span>/lib/tools.jar:/home/hadoop/hadoop/share/hadoop/common/hadoop-common-<span class="number">2.6</span>.<span class="number">0</span>.jar:/home/hadoop/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-<span class="number">2.6</span>.<span class="number">0</span>.jar:/home/hadoop/hadoop/share/hadoop/common/lib/commons-cli-<span class="number">1.2</span>.jar:<span class="variable">$HBASE_JARS</span></span><br></pre></td></tr></table></figure></p>
<p><strong>编译</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ javac <span class="operator">-d</span> classes/ src/*.java</span><br></pre></td></tr></table></figure>
<p><strong>打包</strong></p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ jar -cvf hdfs2hbase<span class="class">.jar</span> classes</span><br></pre></td></tr></table></figure>
<p><strong>运行</strong></p>
<p>创建一个<code>data.txt</code>文件，内容如下（列族是建表时创建的列族<code>cf</code>）：<br><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">r1</span><span class="pseudo">:cf</span><span class="pseudo">:c1</span><span class="pseudo">:value1</span> </span><br><span class="line"><span class="tag">r2</span><span class="pseudo">:cf</span><span class="pseudo">:c2</span><span class="pseudo">:value2</span> </span><br><span class="line"><span class="tag">r3</span><span class="pseudo">:cf</span><span class="pseudo">:c3</span><span class="pseudo">:value3</span></span><br></pre></td></tr></table></figure></p>
<p>将文件复制到hdfs上：<br><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop/bin/hadoop fs -put <span class="typedef"><span class="keyword">data</span>.txt /hbase</span></span><br></pre></td></tr></table></figure></p>
<p>运行MapReduce作业：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop/bin/hadoop jar Hdfs2HBase/hdfs2hbase<span class="class">.jar</span> com<span class="class">.lisong</span><span class="class">.hdfs2hbase</span><span class="class">.Hdfs2HBase</span> /hbase/data<span class="class">.txt</span> mytable</span><br></pre></td></tr></table></figure></p>
<p>报错<code>NoClassDefFoundError</code>找不到类定义：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Exception <span class="keyword">in</span> thread <span class="string">"main"</span> java<span class="class">.lang</span><span class="class">.NoClassDefFoundError</span>: org/apache/hadoop/hbase/io/ImmutableBytesWritable</span><br><span class="line">	at com<span class="class">.lisong</span><span class="class">.hdfs2hbase</span><span class="class">.Hdfs2HBase</span><span class="class">.main</span>(Hdfs2HBase<span class="class">.java</span>:<span class="number">30</span>)</span><br><span class="line">	at sun<span class="class">.reflect</span><span class="class">.NativeMethodAccessorImpl</span><span class="class">.invoke0</span>(Native Method)</span><br><span class="line">	...</span><br><span class="line">	at org<span class="class">.apache</span><span class="class">.hadoop</span><span class="class">.util</span><span class="class">.RunJar</span><span class="class">.run</span>(RunJar<span class="class">.java</span>:<span class="number">221</span>)</span><br><span class="line">	at org<span class="class">.apache</span><span class="class">.hadoop</span><span class="class">.util</span><span class="class">.RunJar</span><span class="class">.main</span>(RunJar<span class="class">.java</span>:<span class="number">136</span>)</span><br><span class="line">Caused by: java<span class="class">.lang</span><span class="class">.ClassNotFoundException</span>: org<span class="class">.apache</span><span class="class">.hadoop</span><span class="class">.hbase</span><span class="class">.io</span><span class="class">.ImmutableBytesWritable</span></span><br><span class="line">	at java<span class="class">.net</span><span class="class">.URLClassLoader</span><span class="class">.findClass</span>(URLClassLoader<span class="class">.java</span>:<span class="number">381</span>)</span><br><span class="line">	at java<span class="class">.lang</span><span class="class">.ClassLoader</span><span class="class">.loadClass</span>(ClassLoader<span class="class">.java</span>:<span class="number">424</span>)</span><br><span class="line">	at java<span class="class">.lang</span><span class="class">.ClassLoader</span><span class="class">.loadClass</span>(ClassLoader<span class="class">.java</span>:<span class="number">357</span>)</span><br><span class="line">	... <span class="number">7</span> more</span><br></pre></td></tr></table></figure></p>
<p>原因是我没有把HBase的jar包加到<code>hadoop-env.sh</code>中。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">TEMP=`ls /home/hadoop/hbase/lib/*.jar`</span><br><span class="line">HBASE_JARS=`<span class="built_in">echo</span> <span class="variable">$TEMP</span> | sed <span class="string">'s/ /:/g'</span>`</span><br><span class="line">HADOOP_CLASSPATH=<span class="variable">$HBASE_JARS</span></span><br></pre></td></tr></table></figure></p>
<p>再次运行发现又报了<code>Unable to initialize MapOutputCollector</code>的错误：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">15</span>/<span class="number">08</span>/<span class="number">10</span> <span class="number">08</span>:<span class="number">55</span>:<span class="number">44</span> WARN mapred<span class="class">.MapTask</span>: Unable to initialize MapOutputCollector org<span class="class">.apache</span><span class="class">.hadoop</span><span class="class">.mapred</span><span class="class">.MapTask</span><span class="variable">$MapOutputBuffer</span></span><br><span class="line">java<span class="class">.lang</span><span class="class">.NullPointerException</span></span><br><span class="line">	at org<span class="class">.apache</span><span class="class">.hadoop</span><span class="class">.mapred</span><span class="class">.MapTask</span><span class="variable">$MapOutputBuffer</span>.<span class="function"><span class="title">init</span><span class="params">(MapTask.java:<span class="number">1008</span>)</span></span></span><br><span class="line">	at org<span class="class">.apache</span><span class="class">.hadoop</span><span class="class">.mapred</span><span class="class">.MapTask</span><span class="class">.createSortingCollector</span>(MapTask<span class="class">.java</span>:<span class="number">401</span>)</span><br><span class="line">	...</span><br><span class="line">	at java<span class="class">.lang</span><span class="class">.Thread</span><span class="class">.run</span>(Thread<span class="class">.java</span>:<span class="number">745</span>)</span><br><span class="line"><span class="number">15</span>/<span class="number">08</span>/<span class="number">10</span> <span class="number">08</span>:<span class="number">55</span>:<span class="number">44</span> INFO mapred<span class="class">.LocalJobRunner</span>: map task executor complete.</span><br><span class="line"><span class="number">15</span>/<span class="number">08</span>/<span class="number">10</span> <span class="number">08</span>:<span class="number">55</span>:<span class="number">44</span> WARN mapred<span class="class">.LocalJobRunner</span>: job_local2138114942_0001</span><br><span class="line">java<span class="class">.lang</span><span class="class">.Exception</span>: java<span class="class">.io</span><span class="class">.IOException</span>: Unable to initialize any output collector</span><br><span class="line">	at org<span class="class">.apache</span><span class="class">.hadoop</span><span class="class">.mapred</span><span class="class">.LocalJobRunner</span><span class="variable">$Job</span>.<span class="function"><span class="title">runTasks</span><span class="params">(LocalJobRunner.java:<span class="number">462</span>)</span></span></span><br><span class="line">	at org<span class="class">.apache</span><span class="class">.hadoop</span><span class="class">.mapred</span><span class="class">.LocalJobRunner</span><span class="variable">$Job</span>.<span class="function"><span class="title">run</span><span class="params">(LocalJobRunner.java:<span class="number">522</span>)</span></span></span><br><span class="line">Caused by: java<span class="class">.io</span><span class="class">.IOException</span>: Unable to initialize any output collector</span><br><span class="line">	at org<span class="class">.apache</span><span class="class">.hadoop</span><span class="class">.mapred</span><span class="class">.MapTask</span><span class="class">.createSortingCollector</span>(MapTask<span class="class">.java</span>:<span class="number">412</span>)</span><br><span class="line">	...</span><br><span class="line">	at java<span class="class">.util</span><span class="class">.concurrent</span><span class="class">.ThreadPoolExecutor</span><span class="variable">$Worker</span>.<span class="function"><span class="title">run</span><span class="params">(ThreadPoolExecutor.java:<span class="number">617</span>)</span></span></span><br><span class="line">	at java<span class="class">.lang</span><span class="class">.Thread</span><span class="class">.run</span>(Thread<span class="class">.java</span>:<span class="number">745</span>)</span><br><span class="line"><span class="number">15</span>/<span class="number">08</span>/<span class="number">10</span> <span class="number">08</span>:<span class="number">55</span>:<span class="number">44</span> INFO mapreduce<span class="class">.Job</span>: Job job_local2138114942_0001 failed with state FAILED due to: NA</span><br><span class="line"><span class="number">15</span>/<span class="number">08</span>/<span class="number">10</span> <span class="number">08</span>:<span class="number">55</span>:<span class="number">45</span> INFO mapreduce<span class="class">.Job</span>: Counters: <span class="number">0</span></span><br></pre></td></tr></table></figure></p>
<p>原因是我没有指明Map输出的Key/Value类型，在<code>Hdfs2HBase.java</code>中添加以下两句：<br><figure class="highlight vbnet"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">job.setMapOutputKeyClass(<span class="keyword">Text</span>.<span class="keyword">class</span>);</span><br><span class="line">job.setMapOutputValueClass(<span class="keyword">Text</span>.<span class="keyword">class</span>);</span><br></pre></td></tr></table></figure></p>
<p>如果没有专门定义Mapper输出类型的话，<code>job.setOutputKeyClass</code>和<code>job.setOutputValueClass</code>设置的是Mapper和Reducer两个的输出类型。<br><figure class="highlight actionscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">job.setOutputKeyClass(ImmutableBytesWritable.<span class="keyword">class</span>);</span><br><span class="line">job.setOutputValueClass(Put.<span class="keyword">class</span>);</span><br></pre></td></tr></table></figure></p>
<p>而Hdfs2HBaseMapper输出类型是Text/Text，所以这里需要单独指定。</p>
<hr>
<p><strong>修改Hdfs2HBase.java</strong><br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.lisong.hdfs2hbase;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.io.ImmutableBytesWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.Put;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.mapreduce.TableOutputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.GenericOptionsParser;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Hdfs2HBase</span> </span>&#123;</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">		Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">		String[] otherArgs = <span class="keyword">new</span> GenericOptionsParser(conf, args).getRemainingArgs();</span><br><span class="line">		<span class="keyword">if</span>(otherArgs.length != <span class="number">2</span>) &#123;</span><br><span class="line">			System.err.println(<span class="string">"Usage: wordcount &lt;infile&gt; &lt;table&gt;"</span>);</span><br><span class="line">			System.exit(<span class="number">2</span>);</span><br><span class="line">		&#125;</span><br><span class="line">		</span><br><span class="line">		Job job = <span class="keyword">new</span> Job(conf, <span class="string">"hdfs2hbase"</span>);</span><br><span class="line">		job.setJarByClass(Hdfs2HBase.class);</span><br><span class="line">		job.setMapperClass(Hdfs2HBaseMapper.class);</span><br><span class="line">		job.setReducerClass(Hdfs2HBaseReducer.class);</span><br><span class="line">		</span><br><span class="line">		job.setMapOutputKeyClass(Text.class);    <span class="comment">// +</span></span><br><span class="line">		job.setMapOutputValueClass(Text.class);  <span class="comment">// +</span></span><br><span class="line">	</span><br><span class="line">		job.setOutputKeyClass(ImmutableBytesWritable.class);</span><br><span class="line">		job.setOutputValueClass(Put.class);</span><br><span class="line">		</span><br><span class="line">		job.setOutputFormatClass(TableOutputFormat.class);</span><br><span class="line">		</span><br><span class="line">		FileInputFormat.addInputPath(job, <span class="keyword">new</span> Path(otherArgs[<span class="number">0</span>]));</span><br><span class="line">		job.getConfiguration().set(TableOutputFormat.OUTPUT_TABLE, otherArgs[<span class="number">1</span>]);</span><br><span class="line">		</span><br><span class="line">		System.exit(job.waitForCompletion(<span class="keyword">true</span>)?<span class="number">0</span>:<span class="number">1</span>);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>再次编译、打包，然后运行成功！</p>
<p>查询HBase表，验证数据是否已导入：<br><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):<span class="number">001</span>:<span class="number">0</span>&gt; scan 'mytable'</span><br><span class="line">ROW                         COLUMN+CELL                                                                 </span><br><span class="line"> r1                         <span class="variable">column=</span>cf:c1, <span class="variable">timestamp=</span><span class="number">1439223857492</span>, <span class="variable">value=</span>value1                         </span><br><span class="line"> r2                         <span class="variable">column=</span>cf:c2, <span class="variable">timestamp=</span><span class="number">1439223857492</span>, <span class="variable">value=</span>value2                         </span><br><span class="line"> r3                         <span class="variable">column=</span>cf:c3, <span class="variable">timestamp=</span><span class="number">1439223857492</span>, <span class="variable">value=</span>value3                         </span><br><span class="line"><span class="number">3</span> row(s) <span class="keyword">in</span> <span class="number">1.3820</span> seconds</span><br></pre></td></tr></table></figure></p>
<p>可以看到，数据导入成功！</p>
<p>由于需要频繁的与存储数据的RegionServer通信，占用资源较大，一次性入库大量数据时，TableOutputFormat效率并不好。</p>
<p><br></p>
<h1 id="二、拓展-TableReducer">二、拓展-TableReducer</h1><p>我们可以将<code>Hdfs2HBaseReducer.java</code>代码改成下面这样，作用是一样的：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.lisong.hdfs2hbase;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Writable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.mapreduce.TableReducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.io.ImmutableBytesWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.Put;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.util.Bytes;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Hdfs2HBaseReducer</span> <span class="keyword">extends</span> <span class="title">TableReducer</span>&lt;<span class="title">Text</span>, <span class="title">Text</span>, <span class="title">ImmutableBytesWritable</span>&gt; </span>&#123;</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text rowkey, Iterable&lt;Text&gt; value, Context context)</span> <span class="keyword">throws</span> IOException,InterruptedException </span>&#123;</span><br><span class="line">		String k = rowkey.toString();</span><br><span class="line">		<span class="keyword">for</span>(Text val : value) &#123;</span><br><span class="line">			Put put = <span class="keyword">new</span> Put(k.getBytes());</span><br><span class="line">			String[] strs = val.toString().split(<span class="string">":"</span>);</span><br><span class="line">			String family = strs[<span class="number">0</span>];</span><br><span class="line">			String qualifier = strs[<span class="number">1</span>];</span><br><span class="line">			String v = strs[<span class="number">2</span>];</span><br><span class="line">			put.add(family.getBytes(), qualifier.getBytes(), v.getBytes());</span><br><span class="line">			context.write(<span class="keyword">new</span> ImmutableBytesWritable(k.getBytes()), put);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里直接继承了<code>TableReducer</code>，<code>TableReducer</code>是部分特例化的<code>Reducer</code>，它只有三个类型参数：输入Key/Value是对应Mapper的输出，输出Key可以是任意的类型，但是输出Value必须是一个<code>Put</code>或<code>Delete</code>实例。</p>
<p><img src="http://img.blog.csdn.net/20150812105912587" alt=""></p>
<p>编译打包运行，结果与前面的一样！</p>
<p><br><br><br><br><br></p>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/HBase/" rel="tag">#HBase</a>
          
            <a href="/tags/HDFS/" rel="tag">#HDFS</a>
          
            <a href="/tags/Hadoop/" rel="tag">#Hadoop</a>
          
            <a href="/tags/MapReduce/" rel="tag">#MapReduce</a>
          
        </div>
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2015/07/29/mapreduce-word-count/" rel="next" title="第一个MapReduce程序——WordCount">
                <i class="fa fa-chevron-left"></i> 第一个MapReduce程序——WordCount
              </a>
            
          </div>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2015/08/15/root-and-meta-table-structure/" rel="prev" title="-ROOT-表和.META.表结构详解">
                -ROOT-表和.META.表结构详解 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
      <div class="ds-thread" data-thread-key="2015/08/13/hdfs-import-to-hbase/"
           data-title="MapReduce将HDFS文本数据导入HBase中" data-url="http://yoursite.com/2015/08/13/hdfs-import-to-hbase/">
      </div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="https://avatars0.githubusercontent.com/u/6904366?v=3&s=140"
               alt="Song Lee" />
          <p class="site-author-name" itemprop="name">Song Lee</p>
          <p class="site-description motion-element" itemprop="description">放宽心，多努力</p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">88</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          
            <div class="site-state-item site-state-categories">
              <a href="/categories">
                <span class="site-state-item-count">17</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">25</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/SongLee24" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://blog.csdn.net/lisonglisonglisong" target="_blank" title="CSDN">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                  CSDN
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://weibo.com/lisonglisong" target="_blank" title="Weibo">
                  
                    <i class="fa fa-fw fa-weibo"></i>
                  
                  Weibo
                </a>
              </span>
            
          
        </div>

        
        
          <div class="cc-license motion-element" itemprop="license">
            <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" target="_blank">
              <img src="/images/cc-by-nc-sa.svg" alt="Creative Commons" />
            </a>
          </div>
        

        
        

      </section>

      
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">
            
              
            
            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#一、示例程序"><span class="nav-number">1.</span> <span class="nav-text">一、示例程序</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#二、拓展-TableReducer"><span class="nav-number">2.</span> <span class="nav-text">二、拓展-TableReducer</span></a></li></ol></div>
            
          </div>
        </section>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy;  2014 - 
  <span itemprop="copyrightYear">2016</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Song Lee</span>
</div>

<div class="powered-by">
  <div style="float:left;margin-top:7px;margin-right:10px;">
	<script type="text/javascript">var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan id='cnzz_stat_icon_1254974724'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s11.cnzz.com/z_stat.php%3Fid%3D1254974724%26show%3Dpic' type='text/javascript'%3E%3C/script%3E"));</script>
  </div>
</div>

<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>

        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/vendors/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.0.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.0.1"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.0.1"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.0.1"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.0.1"></script>



  

  
    
  

  <script type="text/javascript">
    var duoshuoQuery = {short_name:"songlee24"};
    (function() {
      var ds = document.createElement('script');
      ds.type = 'text/javascript';ds.async = true;
      ds.id = 'duoshuo-script';
      ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
      ds.charset = 'UTF-8';
      (document.getElementsByTagName('head')[0]
      || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
  </script>

  
    
    <script src="/vendors/ua-parser-js/dist/ua-parser.min.js?v=0.7.9"></script>
    <script src="/js/src/hook-duoshuo.js"></script>
  






  
  

  

  

  

</body>
</html>
